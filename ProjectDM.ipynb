{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielmec/Data-Mining/blob/main/ProjectDM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><img src=\"https://www.aisa.digital/wp-content/uploads/2020/09/multilingual-chatbot-300x263.png\" width=\"80\" height=\"80\">  <strong>Text Mining and Natural Language Processing Project</strong></h1>\n",
        "<h2><strong>Parameter Efficient Fine-Tuning with QLora and Unsloth applied to a Machine Translation task</strong></h2>\n"
      ],
      "metadata": {
        "id": "ebIO_nKU9mx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project will perform a fine-tuning on a language model with QLora, for parameter efficiency training, and [Unsloth](https://unsloth.ai/), an optimization library for faster training and inference.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nvGEz8vw_gwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "h-a519_oBhcF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT1qCkAVCl2P"
      },
      "outputs": [],
      "source": [
        "# Install core packages\n",
        "!pip install -q datasets \\\n",
        "                torch==2.9.0 \\\n",
        "                transformers \\\n",
        "                trl \\\n",
        "                accelerate \\\n",
        "                bitsandbytes \\\n",
        "                unsloth \\\n",
        "                evaluate \\\n",
        "                bert_score \\\n",
        "                sacrebleu \\\n",
        "                rouge_score \\\n",
        "                nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wj-7GeCY3HEo"
      },
      "source": [
        "> ‼️**RESTART RUNTIME**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Environment"
      ],
      "metadata": {
        "id": "K1ebJhS_CEz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, let's add required import for training and data analysis\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "heVds_zACi5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import unsloth # <--- important to load unsloth at the beginning\n",
        "\n",
        "\n",
        "from trl import SFTConfig, SFTTrainer, GRPOConfig, GRPOTrainer\n",
        "from transformers import TrainingArguments, GenerationConfig\n",
        "from unsloth import is_bfloat16_supported\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "\n",
        "from evaluate import load\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from transformers import set_seed\n",
        "from datasets import load_dataset\n",
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n"
      ],
      "metadata": {
        "id": "kMueKcVIC7lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set a seed for reproducibility to allow reproducible experiments:"
      ],
      "metadata": {
        "id": "E5CQK2e4D-o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "set_seed(SEED)"
      ],
      "metadata": {
        "id": "EiJ5cjWeEDeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define utility functions and data"
      ],
      "metadata": {
        "id": "1UqKWu2SCIda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_from_hf(model_name, max_seq_length = 2048, dtype = None, load_in_4bit = True):\n",
        "    \"\"\"\n",
        "    Loads model and tokenizer from HuggingFace with selected parameters, using Unsloth.\n",
        "    Returns a tuple (model, tokenizer)\n",
        "    \"\"\"\n",
        "    return FastLanguageModel.from_pretrained(\n",
        "        model_name = model_name,\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "\n",
        "def add_lora_adapters(model, r = 16, alpha = 16, lora_dropout = 0, target_layers = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]):\n",
        "    \"\"\"\n",
        "    Adds specified Lora adapters\n",
        "    \"\"\"\n",
        "    return FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r = r,\n",
        "        target_modules = target_layers,\n",
        "        lora_alpha = alpha,\n",
        "        lora_dropout = lora_dropout,            # Supports any, but = 0 is optimized\n",
        "        bias = \"none\",                          # Supports any, but = \"none\" is optimized\n",
        "        use_gradient_checkpointing = \"unsloth\", # for efficiency in gradient backpropagation (uses 30% less VRAM, fits 2x larger batch sizes)\n",
        "        random_state = SEED,\n",
        "        use_rslora = False,                     # enable rank stabilized LoRA\n",
        "        loftq_config = None,                    # And LoftQ\n",
        "    )\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a helpful assistant. Your task is to translate to German the text provided by the user.\n",
        "Answer directly with no prehamble or comment.\n",
        "\"\"\"\n",
        "\n",
        "def generate_outputs_in_batches(model,\n",
        "                                tokenizer,\n",
        "                                dataset,\n",
        "                                batch_size : int = 16,\n",
        "                                max_length : int = 512,\n",
        "                                max_new_tokens : int = 256,\n",
        "                                device : str = \"cuda\"):\n",
        "\n",
        "    # enable native 2x faster inference\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    texts = dataset[\"en\"]\n",
        "\n",
        "    # calculate the number of batches\n",
        "    num_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "\n",
        "    generated_outputs = []\n",
        "    for i in tqdm(range(num_batches)):\n",
        "        this_batch = texts[i * batch_size:(i + 1) * batch_size]\n",
        "\n",
        "        # prompt the data into a conversation-like input\n",
        "        batch_texts = []\n",
        "        for txt in this_batch:\n",
        "          msg = [\n",
        "              {\n",
        "                  \"role\" : \"system\",\n",
        "                  \"content\" : SYSTEM_PROMPT\n",
        "              },\n",
        "              {\n",
        "                  \"role\" : \"user\",\n",
        "                  \"content\" : txt\n",
        "              }\n",
        "          ]\n",
        "          msg_prompted = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
        "          batch_texts.append(msg_prompted)\n",
        "\n",
        "        inputs = tokenizer(batch_texts,\n",
        "                           padding=True,\n",
        "                           return_tensors=\"pt\",\n",
        "                           max_length=max_length,\n",
        "                           truncation=True).to(device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.2,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "        # decode the generated outputs into human-readable text\n",
        "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        # post-process outputs to remove special tokens\n",
        "        parsed_dec_outs = []\n",
        "        for out in decoded_outputs:\n",
        "          parsed_dec_outs.append(out.split(\"assistant\")[-1].strip())\n",
        "\n",
        "        generated_outputs.extend(parsed_dec_outs)\n",
        "\n",
        "    return generated_outputs\n",
        "\n",
        "def display_translations(pre_train_translations, post_train_translations, test_dataset, take = 10):\n",
        "    \"\"\"\n",
        "    Displays the translations in a dataframe, for a visual comparison\n",
        "    \"\"\"\n",
        "    take = min(take, len(pre_train_translations), len(post_train_translations), len(test_dataset['en']))\n",
        "    data = {\n",
        "        \"Original Text\": test_dataset['en'][:take],\n",
        "        \"Baseline Translation\": pre_train_translations[:take],\n",
        "        \"Finetuned Model Translation\": post_train_translations[:take],\n",
        "        \"Gold (Reference)\": test_dataset['de'][:take]\n",
        "    }\n",
        "\n",
        "    with pd.option_context('display.max_colwidth', None):\n",
        "        display(pd.DataFrame(data))\n"
      ],
      "metadata": {
        "id": "9a4MTRW9D5ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "dataset_name = \"bentrevett/multi30k\"\n",
        "completion_kwargs = dict(\n",
        "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\",\n",
        "    response_part=\"<|start_header_id|>assistant<|end_header_id|>\"\n",
        ")"
      ],
      "metadata": {
        "id": "aq9hxFUTKnGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "F7zc2ZiyIdZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BLEU\n",
        "As we're doing a task of machine translation, BLEU is chosen as the main evaluation metric.\n",
        "\n",
        "It evaluates the machine-generated text based on how similar it is to a reference text. It works by measuring how many _n-grams_ (sequences of n words) in the candidate appear in the reference. The _n-gram_ precision is computed for multiple values of _n_ (in practice, almost always 1 to 4), and averaged with the geometric average:\n",
        "\n",
        "$$\\text{BLEU}^* = \\prod_{n=1}^{N} p_n^{w_n}$$\n",
        "\n",
        "However, this formula alone encourages the model to keep the output as short as possible. Suppose to compare the candiate and reference text below:\n",
        "\n",
        "> *Candidate:* Today the\n",
        ">\n",
        "> *Reference:* Today the weather is sunny\n",
        "\n",
        "This example would get a perfect precision (1.0) for both 1-gram precision and 2-gram precision!\n",
        "\n",
        "To avoid the above scenario, BLEU introduces a **brevity penalty (BP)**, which penalizes candidate performance proportionally to how shorter the candidate text is compared to the reference one. The brevity penalty is applied as follows:\n",
        "\n",
        "$$\\text{BP} = \\begin{cases}\n",
        "1 & \\text{if } c > r \\\\\n",
        "e^{(1-r/c)} & \\text{if } c \\leq r\n",
        "\\end{cases}$$\n",
        "\n",
        "Where $c$ is the candidate text length while $r$ the reference text length.\n",
        "Therefore, BLEU's formula looks as follows:\n",
        "\n",
        "$$\\text{BLEU} = \\text{BP} \\cdot \\prod_{n=1}^{N} p_n^{w_n}$$"
      ],
      "metadata": {
        "id": "-2ba6jQnYKkD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SacreBLEU\n",
        "The original BLEU implementation is far from perfect. Although it has been used as the main metric in research papers for machine translation, there are many ways to tweak its evaluation, as explained in the [SacreBLEU](https://arxiv.org/abs/1804.08771) paper:\n",
        "\n",
        "- In standard BLEU, researchers often apply their own tokenization (e.g, splitting punctuation or compounds) to the reference and hypothesis before scoring\n",
        "- In addition to that, BLEU requires a number of parameters that are often not reported in research papers\n",
        "\n",
        "SacreBLEU handles these issues by requiring plaintext instead of pre-tokenized text, and applies its own tokenization, to ensure standardization. It also outputs a version signature string for reproducibility. For these reasons, SacreBLEU is used as a wrapper for standard BLEU in the following experiments."
      ],
      "metadata": {
        "id": "PYvhXltR1sz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Limitations of BLEU\n",
        "Similarly to ROUGE, BLEU is weak agains synonims in translations, as both ROUGE and BLEU use _n-grams_. More precise techniques based on ML models could be used, such as [COMET](https://unbabel.github.io/COMET/html/index.html), which tests the proximity of the candidate and reference text in the latent space. Given the time and resource contraints of Google Colab, SacreBLEU is used for the experiments."
      ],
      "metadata": {
        "id": "ZWODtUlh1vOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sacrebleu = load('sacrebleu')"
      ],
      "metadata": {
        "id": "-q5YoQVNQCW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define some utility functions for evaluating the outputs of the model with SacreBLEU:"
      ],
      "metadata": {
        "id": "815tqyW7QE3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_sacrebleu(data):\n",
        "    \"\"\"\n",
        "    Formats output of sacrebleu in two distinct dataframes\n",
        "    \"\"\"\n",
        "    if isinstance(data, str):\n",
        "        data = json.loads(data)\n",
        "\n",
        "    ngram_df = pd.DataFrame({\n",
        "        'N-Gram': ['1-gram', '2-gram', '3-gram', '4-gram'],\n",
        "        'Counts': data['counts'],\n",
        "        'Totals': data['totals'],\n",
        "        'Precision (%)': data['precisions']\n",
        "    })\n",
        "\n",
        "    summary_data = {\n",
        "        'BLEU Score': [round(data['score'], 4)],\n",
        "        'Brevity Penalty (BP)': [data['bp']],\n",
        "    }\n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "\n",
        "    return ngram_df, summary_df\n",
        "\n",
        "def test_completions_with_sacrebleu(model, tokenizer, test_dataset, batch_size = 8, max_new_tokens = 75):\n",
        "    \"\"\"\n",
        "    Tests a model on generation agains a test_dataset, using SacreBLEU for scoring.\n",
        "    Returns the completions and the performance results\n",
        "    \"\"\"\n",
        "    gen_configs = {\n",
        "        \"model\" : model,\n",
        "        \"tokenizer\" : tokenizer,\n",
        "        \"dataset\" : test_dataset,\n",
        "        \"batch_size\" : batch_size,\n",
        "        \"max_new_tokens\" : max_new_tokens\n",
        "    }\n",
        "\n",
        "    translations = generate_outputs_in_batches(**gen_configs)\n",
        "    results = sacrebleu.compute(\n",
        "        predictions=translations,\n",
        "        references=test_dataset[\"de\"],\n",
        "    )\n",
        "\n",
        "    return translations, results\n"
      ],
      "metadata": {
        "id": "5oi2f5LXQNc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For completeness, let's also use ROUGE to test the main results:"
      ],
      "metadata": {
        "id": "YjemPelt6UeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = load('rouge')\n",
        "\n",
        "def format_rouge(data):\n",
        "    \"\"\"\n",
        "    Formats output of ROUGE into a summary dataframe.\n",
        "    \"\"\"\n",
        "    if isinstance(data, str):\n",
        "        data = json.loads(data)\n",
        "\n",
        "    formatted_data = {\n",
        "        'Metric': [],\n",
        "        'Score': []\n",
        "    }\n",
        "\n",
        "    for metric, score in data.items():\n",
        "        formatted_data['Metric'].append(metric.upper())\n",
        "        formatted_data['Score'].append(round(score, 4))\n",
        "\n",
        "    summary_df = pd.DataFrame(formatted_data)\n",
        "\n",
        "    return summary_df\n",
        "\n",
        "def get_rouge_scores(predictions, test_dataset):\n",
        "    \"\"\"\n",
        "    Tests a model using ROUGE for scoring.\n",
        "    \"\"\"\n",
        "    # ROUGE expectations: predictions (list of strings), references (list of strings or list of list of strings)\n",
        "    return rouge.compute(\n",
        "        predictions=predictions,\n",
        "        references=test_dataset[\"de\"],\n",
        "        use_stemmer=True\n",
        "    )"
      ],
      "metadata": {
        "id": "t-dUpufW6MbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Analysis"
      ],
      "metadata": {
        "id": "8cWaFDwAIvXn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset chosen is [bentrevett/multi30k](https://huggingface.co/datasets/bentrevett/multi30k), a textual adaptation of the [Multi30k](https://arxiv.org/abs/1605.00459) dataset, which consists of images alongside their English descriptions and German translations.\n",
        "\n",
        "\n",
        "Samples were manually annotated by professional\n",
        "English-German translators contracted via an established Language Service in Germany. The translations were done **without** showing the reference image; therefore, the dataset is suitable for standard machine translation."
      ],
      "metadata": {
        "id": "DOFforIHx-p7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(dataset_name)\n",
        "train_dataset, eval_dataset, test_dataset = dataset[\"train\"], dataset[\"validation\"], dataset[\"test\"]\n",
        "dataset"
      ],
      "metadata": {
        "id": "euGBsIk3Q_jC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the name suggest, the dataset contains around 30k samples. Below is provided an example of what samples look like:"
      ],
      "metadata": {
        "id": "TeJLxcG7B3U2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('max_colwidth', None)\n",
        "pd.DataFrame(dataset['train'].select(range(10)))"
      ],
      "metadata": {
        "id": "Nu5r-nb4CUcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several metrics are analyzed on the available data:\n",
        "\n",
        "- **Type-Token Ratio**: it is the ratio computed between the number of the unique words and the number of all words. It is a metric used to tell how repetitive or varied a dataset is.\n",
        "- **Hapax**: Counts the rare (hapax) words, which are the words that appear only once in the whole dataset. In most language datasets, a very high percentage of words are hapaxes.\n",
        "- **Average Word Count** (per sample)\n",
        "- **Average Word Length**\n",
        "- **Max Sample Length**"
      ],
      "metadata": {
        "id": "llmA13PfD0x7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def qualitative_analysis(df):\n",
        "    \"\"\"\n",
        "    Analyzes the dataset in a qualitative way, returning statistics about the variety of the language used\n",
        "    \"\"\"\n",
        "    # type-token ratio and rare words\n",
        "    def get_lexical_stats(text_list):\n",
        "        tokens = \" \".join(text_list).lower().split()\n",
        "        total_tokens = len(tokens)\n",
        "        if total_tokens == 0: return 0, 0\n",
        "\n",
        "        counts = Counter(tokens)\n",
        "        ttr = len(counts) / total_tokens\n",
        "\n",
        "        # hapax Legomena: words occurring only once\n",
        "        rare_percentage = (len([w for w, c in counts.items() if c == 1]) / len(counts)) * 100\n",
        "        return ttr, rare_percentage\n",
        "\n",
        "    ttr_en, rare_en = get_lexical_stats(df['en'].tolist())\n",
        "    ttr_de, rare_de = get_lexical_stats(df['de'].tolist())\n",
        "\n",
        "    stats = {\n",
        "        \"Metric\": [\n",
        "            \"Type-Token Ratio (Diversity)\",\n",
        "            \"Rare Words % (Hapax)\",\n",
        "        ],\n",
        "        \"English (en)\": [\n",
        "            ttr_en,\n",
        "            f\"{rare_en:.1f}%\",\n",
        "        ],\n",
        "        \"German (de)\": [\n",
        "            ttr_de,\n",
        "            f\"{rare_de:.1f}%\",\n",
        "        ]\n",
        "    }\n",
        "    return pd.DataFrame(stats)\n",
        "\n",
        "def quantitative_analysis(df):\n",
        "    \"\"\"\n",
        "    Prints metrics related to samples and words lengths\n",
        "    \"\"\"\n",
        "    df['en_len'] = df['en'].apply(lambda x: len(str(x).split()))\n",
        "    df['de_len'] = df['de'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "    def get_mean_word_length(column):\n",
        "        all_words = \" \".join(column.astype(str)).split()\n",
        "        if not all_words: return 0\n",
        "        return sum(len(word) for word in all_words) / len(all_words)\n",
        "\n",
        "    mean_en_word_len = get_mean_word_length(df['en'])\n",
        "    mean_de_word_len = get_mean_word_length(df['de'])\n",
        "\n",
        "    stats = {\n",
        "        \"Metric\": [\n",
        "            \"Average Word Length\",\n",
        "            \"Average Word Count\",\n",
        "            \"Max Sample Length\"\n",
        "        ],\n",
        "        \"English (en)\": [\n",
        "            mean_en_word_len,\n",
        "            df['en_len'].mean(),\n",
        "            df['en_len'].max()\n",
        "        ],\n",
        "        \"German (de)\": [\n",
        "            mean_de_word_len,\n",
        "            df['de_len'].mean(),\n",
        "            df['de_len'].max()\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.hist(df['en_len'], bins=40, alpha=0.5, label='English', color='blue')\n",
        "    plt.hist(df['de_len'], bins=40, alpha=0.5, label='German', color='orange')\n",
        "    plt.title('Global Samples Length Distribution')\n",
        "    plt.xlabel('Number of Words')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return pd.DataFrame(stats)"
      ],
      "metadata": {
        "id": "gZvyqcyn7tPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_data = pd.concat([pd.DataFrame(dataset[s]) for s in dataset.keys()])\n",
        "qualitative_analysis(all_data)"
      ],
      "metadata": {
        "id": "qJbwv4rTBcgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the percentare of **hapaxes**, it can be noticed how the German data have a much higher percentage of rare words compared to English data. This is expected as German have a much richer vocabulary compared to English, as there are many compound words that express complex concepts.\n",
        "\n",
        "The **Type-Token Ratio** confirms this, as it's almost double for German descriptions compared to English ones, meaning that the choice of words for the German descriptions is less repetitive. It could also be due to the expertise of the human annotators that translated the descriptions."
      ],
      "metadata": {
        "id": "NnShRSfOFjP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantitative_analysis(all_data)"
      ],
      "metadata": {
        "id": "TmwIvGFJA3O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the maximum sentence length is sligthly higher for the German dataset, the **average word count** for sample is lower in German. This result confirms again the usage of words in the German language that symbolize complex concepts, that need to be translated with multiple words in English.\n",
        "\n",
        "In fact, looking at the **average word length**, we notice how the average length for words in the German samples is ~30% higher compared to the English ones, which is due to the fact that compound words are more common in German.\n",
        "\n"
      ],
      "metadata": {
        "id": "k-Ow4JfIIW4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Separate Splits Analysis\n",
        "For completeness, the same analysis is repeated for each split separatly. It can be noticed how, in validation and test splits, the values in the histogram are more sparse and fluctuating as the number of samples is more limited."
      ],
      "metadata": {
        "id": "YMtDdcxgN70W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for split in dataset.keys():\n",
        "    print(\"-\" * 5 + f\" {split} split analysis \" + \"-\" * 5)\n",
        "    df = pd.DataFrame(dataset[split])\n",
        "    display(qualitative_analysis(df))\n",
        "    display(quantitative_analysis(df))"
      ],
      "metadata": {
        "id": "EjPWK8VnN29N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supervised Fine-Tuning"
      ],
      "metadata": {
        "id": "nLqN3A17PgF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Supervised Fine-Tuning (SFT)** is the traditional approach where we train the model on labeled data:\n",
        "\n",
        "**How it works:**\n",
        "- The model learns to generate summaries by minimizing the **cross-entropy loss** between predicted and target tokens\n",
        "- We provide pairs of (input sentence, target translation) as training examples\n",
        "- The model adjusts its weights to better predict the target translations\n",
        "- This is a **direct optimization** approach - the model learns to mimic the training data\n",
        "\n",
        "**QLoRA** provides the following additions to the finetuning process:\n",
        "- Adds 4-bit quantization on top of classical SFT, to reduce memory footprint\n",
        "- Makes use of **LoRA adapters**, a set of separable weights that are trained while keeping the rest of the model frozen\n",
        "\n",
        "<img src=\"https://miro.medium.com/0*2meitaJ7pdUusbb5.png\">\n",
        "\n",
        "LoRA introduces a pair of rank-decomposition matrices $A$ and $B$ that are trained while keeping the model's parameter frozen. These matrices are set so that their multiplication has the same dimensions as the weights’ matrix they are intended to modify. $A$ and $B$ are then updated during the fine-tuning process while the model’s weights remain frozen, and they are utilized to compute a $\\Delta W$ matrix that contains the fine-tuning adjustments. During inference, $A$ and $B$ are multiplied, and their product is summed up in the $W$ matrix, effectively updating the model’s weights.\n",
        "\n",
        "\n",
        "\n",
        "Note that the explanation above is a simplification aimed for the theorical explanation, and that models usually have multiple sets of small matrices (the LoRA adapters), that can be trained separately."
      ],
      "metadata": {
        "id": "AuxhXIw5uHZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by loading the model and its tokenizer. The model chosen for the project is the 3B version of **Llama-3.2** instruct version. The base versions of models are pre-trained on massive amount of data (the internet, books, etc.), but they are not suited for instruction following tasks. To better follow commands and answer questions, these models are then finetuned for instruction following."
      ],
      "metadata": {
        "id": "wsCtNp9fB0Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = load_model_from_hf(model_name)\n",
        "print(f\"Model loaded: {model.config.model_type}\")\n",
        "print(f\"Vocab size: {len(tokenizer)}\")"
      ],
      "metadata": {
        "id": "cS1CLfhYPjj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate the baseline performance of the model:"
      ],
      "metadata": {
        "id": "eijFM05yCvRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "original_model_translations, original_model_results = test_completions_with_sacrebleu(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    test_dataset\n",
        ")\n",
        "print(f\"ORIGINAL MODEL STATS:\")\n",
        "n_gram_df, summary_df = format_sacrebleu(original_model_results)\n",
        "display(n_gram_df)\n",
        "display(summary_df)"
      ],
      "metadata": {
        "id": "7rr56USOU2sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the baseline BLEU score is around 27, which marks a clear gist but often with significant errors.\n",
        "\n",
        "Let's also show ROUGE results for completeness:"
      ],
      "metadata": {
        "id": "5Q-ywgE6EQJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "format_rouge(get_rouge_scores(original_model_translations, test_dataset))"
      ],
      "metadata": {
        "id": "Vh775vQY60_v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now prepare for the finetuning and add LoRA adapters, to train just a small amount of the weights compared to the amount of trainable parameters:"
      ],
      "metadata": {
        "id": "lr9KJyRr61SU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = add_lora_adapters(model)\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "all_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"All parameters: {all_params:,}\")\n",
        "print(f\"Percentage trainable: {100 * trainable_params / all_params:.2f}%\")"
      ],
      "metadata": {
        "id": "Le-WTtx_VO_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's now time to format the data samples using the format used by Llama-3.2. When working with instruct models, that are finetuned using a specific prompt format, it's better to stick to the same format when performing additonal finetuning for specific tasks."
      ],
      "metadata": {
        "id": "t5VlVnuwFl1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "def parse_completion_dataset(example : dict) -> str:\n",
        "    \"\"\"\n",
        "    Combine and prompt data instances by merging the original text and the target summary using a simple prompt.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\" : \"system\",\n",
        "            \"content\" : SYSTEM_PROMPT\n",
        "        },\n",
        "        {\n",
        "            \"role\" : \"user\",\n",
        "            \"content\" : example[\"en\"]\n",
        "        },\n",
        "        {\n",
        "            \"role\" : \"assistant\",\n",
        "            \"content\" : example[\"de\"]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    return {\"text\" : tokenizer.apply_chat_template(messages, tokenize=False)}\n",
        "\n",
        "cols = dataset.column_names\n",
        "\n",
        "completion_ds = DatasetDict({\n",
        "    split : dataset[split].map(parse_completion_dataset, remove_columns=cols[split])\n",
        "    for split in dataset\n",
        "})\n",
        "\n",
        "completion_ds"
      ],
      "metadata": {
        "id": "_1oD7PhUVh_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at an example from the completion dataset:"
      ],
      "metadata": {
        "id": "_RZ2X6jtVh_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completion_ds[\"train\"][0]"
      ],
      "metadata": {
        "id": "Ctb2VotqVh_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's now time to begin the finetuning. As Colab offers a limited amount of time and system resources, finetuning is done with a limited amount of steps, as follows:"
      ],
      "metadata": {
        "id": "hYF5WI_FVh_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# > Training Config\n",
        "LR = 2e-4\n",
        "TRAIN_BS=4\n",
        "EVAL_BS=8\n",
        "GRAD_ACC_STEPS=4\n",
        "WARMUP_STEPS=5\n",
        "WEIGHT_DECAY=0.01\n",
        "\n",
        "MAX_STEPS=200"
      ],
      "metadata": {
        "id": "mjfRM4AbVh_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = completion_ds[\"train\"],\n",
        "    eval_dataset = completion_ds[\"validation\"],\n",
        "    args = SFTConfig(\n",
        "        per_device_train_batch_size = TRAIN_BS,\n",
        "        per_device_eval_batch_size = EVAL_BS,\n",
        "        gradient_accumulation_steps = GRAD_ACC_STEPS,\n",
        "        warmup_steps = WARMUP_STEPS,\n",
        "        max_steps = MAX_STEPS,\n",
        "        learning_rate = LR,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = WEIGHT_DECAY,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = SEED,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "UP6y0tVVVh_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we're only interested in training on the **output** of the model (i.e. the translations), the questions are masked and only the output tokens are considered for the loss function."
      ],
      "metadata": {
        "id": "Vxce3sl5C0mG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3rUWOqmVh_b"
      },
      "outputs": [],
      "source": [
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    **completion_kwargs,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW7mAaGAVh_c"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now compare the finetuned model with the baseline one, by computing its BLEU score on the test set:"
      ],
      "metadata": {
        "id": "qdnFdNvmVh_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model_translations, finetuned_model_results = test_completions_with_sacrebleu(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    test_dataset\n",
        ")\n",
        "print(f\"FINETUNED MODEL STATS:\")\n",
        "n_gram_df, summary_df = format_sacrebleu(finetuned_model_results)\n",
        "display(n_gram_df)\n",
        "display(summary_df)"
      ],
      "metadata": {
        "id": "NRrVU4RqVh_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It can be noticed how a finetuning on a small amount of parameters performed for 200 iterations results in an increase of around 33%.\n",
        "\n",
        "To make sense of these results, a rough guideline of  intrepretation of BLEU scores is the following one:\n",
        "\n",
        "| BLEU Score | Interpretation |\n",
        "| :--- | :--- |\n",
        "| **< 10** | Almost useless |\n",
        "| **10 - 19** | Hard to get the gist |\n",
        "| **20 - 29** | The gist is clear, but has significant grammatical errors |\n",
        "| **30 - 40** | Understandable to good translations |\n",
        "| **40 - 50** | High quality translations |\n",
        "| **50 - 60** | Very high quality, adequate, and fluent translations |\n",
        "| **> 60** | Quality often better than human |\n",
        "\n",
        "The improvement obtained moves the performance of the model of one band, from \"The gist is clear, but has significant grammatical errors\" to \"Understandable to good translations\". Not a bad results compared to the amount of time needed for the training!\n",
        "\n",
        "Let's also display the ROUGE scores:"
      ],
      "metadata": {
        "id": "Es4y3EyjGteI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "format_rouge(get_rouge_scores(finetuned_model_translations, test_dataset))"
      ],
      "metadata": {
        "id": "aBqbycYb77ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be noticed, ROUGE scores improved as well: ROUGE1 improved by roughly 10%, ROUGE2 by 20%, ROUGEL by 10% and ROUGELSUM by 10%.\n",
        "\n",
        "Let's also display the english sentences alongside the baseline translations, the one obtained after finetuning and the reference translations:"
      ],
      "metadata": {
        "id": "ZFtcd47r78Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_translations(original_model_translations, finetuned_model_translations, test_dataset)"
      ],
      "metadata": {
        "id": "IvM0IuEZpWaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRPO Training"
      ],
      "metadata": {
        "id": "ScVNJR4dX9AS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note: the following paragraph was composed starting from the explaination present in the fine-tuning laboratory seen during the course. The explanation was slightly rephrased and modified with details based on my understanding of the papers, which are focused on intuition rather than mathematical precision. Therefore, all the errors in the following text should be considered my additions.\n",
        "\n",
        "While Supervised Fine-Tuning (SFT) directly learns from labeled examples, **Group Relative Policy Optimization (GRPO)** is a reinforcement learning-based approach that optimizes the model based on relative preferences within groups of responses.\n",
        "\n",
        "**Key differences from SFT:**\n",
        "- **SFT**: Learns to mimic exact reference translations through supervised learning\n",
        "- **GRPO**: Learns to generate better translations by comparing multiple candidates and optimizing for quality metrics through reinforcement learning\n",
        "\n",
        "---\n",
        "\n",
        "#### What is GRPO?\n",
        "\n",
        "**Group Relative Policy Optimization** is a reinforcement learning algorithm designed for training large language models, introduced in the paper [*DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models*](https://huggingface.co/papers/2402.03300) (2024).\n",
        "\n",
        "GRPO is a variant of Proximal Policy Optimization (PPO) that enhances mathematical reasoning capabilities while optimizing memory usage. The key innovation is its **group-relative advantage calculation**, which normalizes rewards across multiple completions generated for the same prompt, aligning with the comparative nature of reward models.\n",
        "\n",
        "**Why GRPO over PPO?** GRPO removes the dependency on the value model, a neural network that evaluates the goodness of being in a specific state, which becomes inaccurate for long text outputs and complex tasks. This makes GRPO more scalable and efficient. As the value model is used to compute the advantage over a baseline, the concept of baseline is effectively replaced by the comparison with other completions.\n",
        "\n",
        "---\n",
        "\n",
        "#### How GRPO Works\n",
        "\n",
        "GRPO is an **online learning algorithm** that improves iteratively through four main steps:\n",
        "\n",
        "##### 1. **Generating Completions**\n",
        "For each prompt $x$, the model generates $K$ different completions: $\\{y_1, y_2, \\ldots, y_K\\}$\n",
        "\n",
        "##### 2. **Computing the Advantage**\n",
        "The advantage for each completion is calculated using group normalization:\n",
        "\n",
        "$$\\hat{A_i} = \\frac{r_i - \\text{mean}(r)}{\\text{std}(r)}$$\n",
        "\n",
        "where $r_i$ is the reward for completion $i$, and the mean and standard deviation are computed across all $K$ completions for the same prompt.\n",
        "\n",
        "> **Note:** The original formulation includes division by $\\text{std}(r)$, but recent research suggests that disabling this scaling (by setting `scale_rewards=False`) can reduce question-level difficulty bias.\n",
        "\n",
        "##### 3. **Estimating KL Divergence**\n",
        "KL divergence between the current policy $\\pi_\\theta$ and reference policy $\\pi_{\\text{ref}}$ is estimated to prevent the model from deviating too far from the reference.\n",
        "\n",
        "##### 4. **Computing the Loss**\n",
        "The GRPO objective maximizes the advantage while constraining the policy update using a clipped surrogate objective:\n",
        "\n",
        "$$\n",
        "ℒ_{\\text{GRPO}}(\\theta) = -\\mathbb{E} \\left[ \\min \\left( r_t(\\theta)\\hat{A_~}, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A_~} \\right) \\right] + \\beta \\cdot D_{\\text{KL}}(\\pi_\\theta || \\pi_{\\text{ref}})\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $r_t(\\theta) = \\frac{\\pi_\\theta(y_t|x, y_{<t})}{\\pi_{\\text{ref}}(y_t|x, y_{<t})}$ is the **importance sampling ratio** (token-level)\n",
        "- $\\epsilon$ is the clipping parameter (typically 0.2)\n",
        "- $\\beta$ is the KL coefficient (often set to 0 in practice)\n",
        "\n",
        "The ratio $r_t(\\theta)$ allows us to evaluate the new policy using data from the old policy, avoiding expensive resampling. However, if the gap between policies is too large, this ratio can have high variance, leading to instability.\n",
        "\n",
        "This ratio is clipped to ensure the proximity policy, as a very high reward would encourage the model to explore more \"extreme\" paths, leading to instable behaviors.\n",
        "\n",
        "![GRPO Visual](https://huggingface.co/datasets/trl-lib/documentation-images/resolve/main/grpo_visual.png)\n",
        "\n",
        "---\n",
        "\n",
        "#### GRPO's Limitations\n",
        "\n",
        "While GRPO improves upon PPO, it has several key limitations that motivated further research. The following limitations are addressed by DAPO, a more stable version of GRPO that was therefore chosen for the experiments:\n",
        "\n",
        "1. **Response-Level Length Bias**: Longer responses are under-penalized in the original formulation\n",
        "2. **Inefficient Sampling**: When all samples have similar rewards (all good or all bad), gradient contributions become zero\n",
        "3. **Token Gradient Dilution**: In long sequences, individual token gradients get diluted\n",
        "\n",
        "---\n",
        "The [**DAPO paper**](https://huggingface.co/papers/2503.14476) (*DAPO: An Open-Source LLM Reinforcement Learning System at Scale*) introduced four key improvements to address GRPO's efficiency and stability issues.\n",
        "\n",
        "#### **DAPO Improvement #1: Clip-Higher**\n",
        "\n",
        "**The Problem:** GRPO's symmetric clipping range $[1-\\epsilon, 1+\\epsilon]$ causes \"good tokens being capped too early.\" When the old policy assigns very low probability to a good token (positive advantage), the current policy has little room to increase its probability before being clipped.\n",
        "\n",
        "**Example:** If the old policy probability is 0.2 and $\\epsilon=0.2$, the upper bound is $0.2 \\times 1.2 = 0.24$. Even if the current policy increases it to 0.4 (a good improvement!), it gets clipped.\n",
        "\n",
        "**The Solution:** DAPO raises the upper clipping bound while keeping the lower bound fixed, giving the model more room to reinforce good tokens that were previously unlikely.\n",
        "\n",
        "---\n",
        "\n",
        "#### **DAPO Improvement #2: Dynamic Sampling**\n",
        "\n",
        "**The Problem:** When all sampled responses have the same reward (all 0 or all 1), they all receive zero advantage after normalization, contributing zero gradient. This wastes computation and increases variance.\n",
        "\n",
        "**The Solution:** Enforce that for each query, the sampled responses must contain both correct and incorrect answers:\n",
        "\n",
        "$$\\exists i, j: r_i \\neq r_j$$\n",
        "\n",
        "If all samples have identical rewards, continue sampling until diversity is achieved.\n",
        "\n",
        "---\n",
        "\n",
        "#### **DAPO Improvement #3: Token-Level Gradient Loss**\n",
        "\n",
        "**The Problem:** The original GRPO normalizes loss by individual sequence length:\n",
        "\n",
        "$$ℒ_{\\text{GRPO-original}}(\\theta) = -\\frac{1}{|y|}\\sum_{t=1}^{|y|}\\min(r_t(\\theta) \\hat{A_~} , \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A_~})$$\n",
        "\n",
        "This causes longer responses to be under-penalized. A 200-token response gives each token weight $\\frac{1}{200}$, while a 10-token response gives each token weight $\\frac{1}{10}$, making shorter responses dominate the gradient.\n",
        "\n",
        "As the DAPO paper states, the original GRPO algorithm employs a sample-level loss calculation, which involves first averaging the losses by token within each sample and then aggregating the losses across samples. In this approach, each sample is assigned an equal weight in the final loss computation.\n",
        "\n",
        "Since all samples are assigned the same weight in the loss calculation, tokens within longer responses (which contain more tokens) may have a disproportionately lower contribution to the overall loss, which can lead to two adverse effects. First, for high-quality long samples, this effect can impede the model’s ability to learn\n",
        "reasoning-relevant patterns within them. Second, excessively long samples often exhibit low-quality patterns such as gibberish and repetitive words.\n",
        "\n",
        "**The Solution:** DAPO proposes **token-level normalization** across the entire batch:\n",
        "\n",
        "$$ℒ_{\\text{DAPO}}(\\theta) = -\\frac{1}{\\sum_i |y_i|}\\sum_{i}\\sum_{t=1}^{|y_i|}\\min(r_t^{(i)}(\\theta)\\hat{A_~}_i, \\text{clip}(r_t^{(i)}(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A_~}_i)$$\n",
        "\n",
        "This treats all tokens equally regardless of response length, preventing gradient signals from being diluted in long, high-quality responses.\n",
        "\n",
        "---\n",
        "\n",
        "#### **DAPO Improvement #4: Overlong Reward Shaping**\n",
        "\n",
        "**The Problem:** Overly long responses can receive high rewards even when they contain unnecessary verbosity or repetition.\n",
        "\n",
        "**The Solution:** Apply a soft penalty to tokens once the sequence exceeds a predefined length threshold. The penalty increases linearly, and if a second threshold is exceeded, the penalty can cancel out the original reward entirely.\n",
        "\n",
        "**Usage in TRL:** Set `loss_type=\"bnpo\"` (default in the current implementation)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ro98fQfRi9OC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's perform another training process with GRPO. Firstly, load again the model:"
      ],
      "metadata": {
        "id": "UxJW3HCMeWAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grpo_model, grpo_tokenizer = load_model_from_hf(model_name)\n",
        "grpo_model = add_lora_adapters(grpo_model)"
      ],
      "metadata": {
        "id": "ScKjdzTZeIV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, prepare the data for the GRPO training. As can be seen by the training sample printed below, the format is slightly different compared to the one used in Supervised Fine-Tuning, as GRPO needs the gold answer separately from the prompt passed to the model, in order to compute the rewards."
      ],
      "metadata": {
        "id": "c6uk2UzJeuPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_grpo_dataset(example):\n",
        "    \"\"\"\n",
        "    Prepare dataset for GRPO training.\n",
        "    GRPO needs prompts (queries) and reference completions for reward calculation.\n",
        "    \"\"\"\n",
        "    msg = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": SYSTEM_PROMPT\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": example[\"en\"]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"prompt\": msg,\n",
        "        \"reference\": example[\"de\"]  # Gold standard for reward calculation\n",
        "    }\n",
        "\n",
        "# Prepare GRPO dataset\n",
        "grpo_dataset = dataset[\"train\"].map(prepare_grpo_dataset, remove_columns=dataset[\"train\"].column_names)\n",
        "grpo_eval_dataset = dataset[\"validation\"].map(prepare_grpo_dataset, remove_columns=dataset[\"validation\"].column_names)\n",
        "\n",
        "print(f\"GRPO Training samples: {len(grpo_dataset)}\")\n",
        "print(f\"GRPO Validation samples: {len(grpo_eval_dataset)}\")\n",
        "print(\"\\nExample query:\")\n",
        "print(json.dumps(grpo_dataset[0], indent=2))"
      ],
      "metadata": {
        "id": "7agGXv8Oesuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now define the reward functions. The first one simply computes the BLEU score on the model's completions:"
      ],
      "metadata": {
        "id": "gWMFpUGqe8YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sacrebleu_reward_function(completions,\n",
        "                          reference,**kwargs):\n",
        "    res = []\n",
        "    for i in range(len(completions)):\n",
        "        res.append(sacrebleu.compute(\n",
        "            predictions=[completions[i][0][\"content\"]],\n",
        "            references=[reference[i]],\n",
        "        )[\"score\"] / 100)\n",
        "    return res"
      ],
      "metadata": {
        "id": "VnPuMyHkfJwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRPO can be used to enforce specific high-level properties of the output, such as the output length. To make use of this feature, let's also define a reward function that penalizes the model based on the difference of its output compared to the gold reference.\n",
        "\n",
        "As the BLEU metric penalizes too short translations itself, let's only penalize too long ones."
      ],
      "metadata": {
        "id": "ekCYW5qJJwjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Length Statistics ---\")\n",
        "english_token_counts = [len(grpo_tokenizer.tokenize(example[\"en\"])) for example in tqdm(dataset[\"train\"])]\n",
        "german_token_counts = [len(grpo_tokenizer.tokenize(example[\"de\"])) for example in tqdm(dataset[\"train\"])]\n",
        "\n",
        "print(f\"English - Average tokens: {np.mean(english_token_counts):.2f}\")\n",
        "print(f\"English - Minimum tokens: {np.min(english_token_counts)}\")\n",
        "print(f\"English - Maximum tokens: {np.max(english_token_counts)}\")\n",
        "\n",
        "print(f\"\\nGerman - Average tokens: {np.mean(german_token_counts):.2f}\")\n",
        "print(f\"German - Minimum tokens: {np.min(german_token_counts)}\")\n",
        "print(f\"German - Maximum tokens: {np.max(german_token_counts)}\")"
      ],
      "metadata": {
        "id": "ygdSW5nzfM2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def length_reward_function(completions, reference, **kwargs):\n",
        "    \"\"\"\n",
        "    Compute length-based reward with bounded exponential decay.\n",
        "    \"\"\"\n",
        "    penalized_rewards = []\n",
        "\n",
        "    for i, completion in enumerate(completions):\n",
        "        ref = reference[i]\n",
        "        reference_length = len(grpo_tokenizer.tokenize(ref, return_tensors=\"np\"))\n",
        "\n",
        "        completion_length = len(grpo_tokenizer.tokenize(completion[0][\"content\"], return_tensors=\"np\"))\n",
        "        length_diff = max(0, completion_length - reference_length)\n",
        "\n",
        "        decay_rate = 0.5\n",
        "        normalized_diff = length_diff / reference_length\n",
        "\n",
        "        reward = math.exp(-decay_rate * normalized_diff)\n",
        "        penalized_rewards.append(reward)\n",
        "\n",
        "    return penalized_rewards"
      ],
      "metadata": {
        "id": "wB-2E430g5vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now load the GRPO Trainer from Unsloth and proceed with the training. As for SFT, training is done with a limited amount of steps."
      ],
      "metadata": {
        "id": "m5BFJKTXLE1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRPO Training Configuration\n",
        "grpo_config = GRPOConfig(\n",
        "    output_dir=\"./grpo_outputs\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=5e-6,             # Lower LR for RL fine-tuning\n",
        "    max_steps=500,                   # Limited for demonstration\n",
        "    logging_steps=1,\n",
        "\n",
        "    # GRPO-specific parameters\n",
        "    num_generations=4,  # Generate 4 candidates per prompt for comparison (the higher the better - more exploration)\n",
        "    max_completion_length=76,  # Maximum translation length\n",
        "    temperature=0.9,    # Sampling temperature for generation\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.2,\n",
        "    generation_kwargs={\n",
        "      \"min_new_tokens\" : 1,\n",
        "      \"use_cache\" : True, # use KV Cache\n",
        "    },\n",
        "\n",
        "    # GRPO config (DAPO)\n",
        "    loss_type=\"bnpo\",\n",
        "    beta=0.0,\n",
        "    epsilon_high=0.28, # recommended by the DAPO paper\n",
        "\n",
        "    # Optimization\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    optim=\"adamw_8bit\",\n",
        "\n",
        "    seed=SEED,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "print(\"GRPO Configuration:\")\n",
        "print(f\"  Batch size: {grpo_config.per_device_train_batch_size}\")\n",
        "print(f\"  Learning rate: {grpo_config.learning_rate}\")\n",
        "print(f\"  Generations per prompt: {grpo_config.num_generations}\")\n",
        "print(f\"  Max steps: {grpo_config.max_steps}\")"
      ],
      "metadata": {
        "id": "4v18GHqghCht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GRPO Trainer\n",
        "grpo_trainer = GRPOTrainer(\n",
        "    model=grpo_model,\n",
        "    args=grpo_config,\n",
        "    train_dataset=grpo_dataset,\n",
        "    tokenizer=grpo_tokenizer,\n",
        "    reward_funcs=[\n",
        "        sacrebleu_reward_function,\n",
        "        length_reward_function\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"GRPO Trainer initialized successfully!\")"
      ],
      "metadata": {
        "id": "6-K5Gv1NhFDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train with GRPO\n",
        "grpo_trainer_stats = grpo_trainer.train()\n",
        "\n",
        "print(\"GRPO Training completed!\")\n",
        "print(grpo_trainer_stats)"
      ],
      "metadata": {
        "id": "xjOMsCKKhIVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now evaluate the model finetuned with GRPO, using the same test split that we've used in SFT:"
      ],
      "metadata": {
        "id": "kF0YVyopLV_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grpo_model_translations, grpo_model_results = test_completions_with_sacrebleu(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    test_dataset\n",
        ")\n",
        "print(f\"GRPO MODEL STATS:\")\n",
        "n_gram_df, summary_df = format_sacrebleu(grpo_model_results)\n",
        "display(n_gram_df)\n",
        "display(summary_df)"
      ],
      "metadata": {
        "id": "58IoQjkJhPwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see how the results is comparable to the one obtained with Supervised Fine-Tuning. Once again, a small training resulted in an increase, of around 33% on the BLEU score in this case.\n",
        "\n",
        "Although not trained directly on them, let's also take a look at the ROUGE scores:"
      ],
      "metadata": {
        "id": "UiMq_JMnLnKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "format_rouge(get_rouge_scores(grpo_model_translations, test_dataset))"
      ],
      "metadata": {
        "id": "Ar77JoqV8Qq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be noticed, although GRPO didn't train specifically on ROUGE, it still shows an increase compared to the base model. Specifically, ROUGE1 increased by 10%, ROUGE2 by 20%, ROUGEL by 10% and ROUGELSUM by 10%, similarly to the previous SFT finetuning.\n",
        "\n",
        "Let's also display for the GPRO model the english sentences alongside the baseline translations, the translations obtained from inference after finetuning and the reference translations:"
      ],
      "metadata": {
        "id": "YDM7g2hB8J9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_translations(original_model_translations, grpo_model_translations, test_dataset)"
      ],
      "metadata": {
        "id": "WmYp5XcfoVN8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}